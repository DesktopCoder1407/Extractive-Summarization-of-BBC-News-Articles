# Extractive Summarization of BBC News Articles Using Naive Bayes and Frequency Driven Topic Representation
News is an essential part of the average American's life. From morning until evening, Americans ingest large quantities of different news topics. As the shift to a more digitized world ever increases, users can obtain their news from an increasing number of sources such as Newspapers, Radio, and Television. However, the most popular form of receiving news today is through Digital Media. The invention of digital media changed the news landscape forever and created new paths for all types of news outlets. Unfortunately, the generation of new media outlets lead to an excessive amount of text to read to understand the daily news: more so than someone can read in a day. An interesting solution to this problem is through text summarization. Instead of reading every article to learn important news, an algorithm can summarize the most important parts of the article to shorten a long, multi-paragraph article to a single easy to read paragraph. Current summarization models use multiple methods: Naive Bayes classifiers to identify ideal summary sentences based on a set of features, weighting sentences based on the summed TF-IDF score of the sentence, and many more not covered in this project. As one related model shows, a Naive Bayes classifier can produce an accurate summarization (defined as a summarization that would be produced by a professional) 84\% of the time. An additional baseline shows that a TF-IDF model can produce a summarization with a ROUGE-1 score of 0.374. This project implements these two summarization models and compares their efficiencies and accuracies on a BBC News dataset. This dataset consists of 2,225 articles with an average of 18 sentences each that are split into multiple categories. The results from this project, evaluated using the ROUGE-2 $F_1$ Score metric, show that a Naive Bayes classifier can produce a summarization with an $F_1$ score of 0.5429 while a TF-IDF summarizer can produce a summarization with an $F_1$ score of 0.4302. Additionally, both models take less than 10 seconds to fully train.